{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune and deploy a custom Generative Command or Command-light model\n",
    "\n",
    "This sample notebook shows you how to finetune and deploy a custom Command or Command-light model using Amazon SageMaker.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "## Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to the packages for [Cohere Command Finetuning](https://aws.amazon.com/marketplace/pp/prodview-oi7awyyaywtzq) and [Cohere Command-Light Finetuning](https://aws.amazon.com/marketplace/pp/prodview-4kbovmir2j56y). If so, skip step: [Subscribe to the finetune algorithm](#1.-Subscribe-to-the-finetune-algorithm)\n",
    "\n",
    "## Contents:\n",
    "1. [Subscribe to the finetune algorithm](#1.-Subscribe-to-the-finetune-algorithm)\n",
    "2. [Finetune Generation Models](#2.-Finetune-the-model)\n",
    "   1. [Upload training and evaluation datasets](#A.-Upload-training-and-evaluation-datasets)\n",
    "   2. [Finetune models on uploaded data](#B.-Finetune-model-on-uploaded-data)\n",
    "3. [Create an endpoint for inference with the custom model](#3.-Create-an-endpoint-for-inference-with-the-custom-model)\n",
    "   1. [Create an endpoint]()\n",
    "   2. [Perform real-time inference]()\n",
    "4. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the endpoint](#A.-Delete-the-endpoint)\n",
    "    2. [Unsubscribe to the listing (optional)](#B.-Unsubscribe-to-the-listing-(optional))\n",
    "    \n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the finetune algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the model algorithm:\n",
    "1. Open the algorithm listing page [Cohere Command Finetuning](https://aws.amazon.com/marketplace/pp/prodview-oi7awyyaywtzq) and [Cohere Command-Light Finetuning](https://aws.amazon.com/marketplace/pp/prodview-4kbovmir2j56y)\n",
    "2. On the AWS Marketplace listing, click on the **Continue to Subscribe** button.\n",
    "3. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms. \n",
    "4. Once you click on **Continue to configuration** button and then choose a **region**, you will see a **Product Arn** displayed. This is the algorithm ARN that you need to specify while creating a finetune or deploying the finetuned model as an endpoint using boto3. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U cohere-sagemaker\n",
    "\n",
    "from cohere_sagemaker import Client\n",
    "import boto3\n",
    "import sagemaker as sage\n",
    "from sagemaker.s3 import S3Uploader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is available on most AWS regions. Add/update the algorithm ARN for your region to this list.\n",
    "\n",
    "**Note**: The ARNs mentioned in this map below default to that for Command-light finetuning. You must use the algorithm for [Command](https://aws.amazon.com/marketplace/pp/prodview-oi7awyyaywtzq) if you want to finetune the Command model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "\n",
    "# TODO Add/update the algorithm ARN for your region to this list.\n",
    "algorithm_map = {\n",
    "    \"us-west-2\": \"arn:aws:sagemaker:us-west-2:594846645681:algorithm/cohere-command-light-tfew-ft-65699db4276c3fba866282b39a6c2a54\",\n",
    "    \"us-west-1\": \"arn:aws:sagemaker:us-west-1:382657785993:algorithm/cohere-command-light-tfew-ft-65699db4276c3fba866282b39a6c2a54\",\n",
    "    \"eu-east-1\": \"arn:aws:sagemaker:us-east-1:865070037744:algorithm/cohere-command-light-tfew-ft-65699db4276c3fba866282b39a6c2a54\",\n",
    "}\n",
    "if region not in algorithm_map.keys():\n",
    "    raise Exception(f\"Current boto3 session region {region} is not supported.\")\n",
    "\n",
    "arn = algorithm_map[region]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Upload training and evaluation datasets\n",
    "\n",
    "Select a path on S3 to store the training and evaluation datasets and update the **s3_data_dir** below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_dir = \"s3://...\"  # Do not add a trailing slash otherwise the upload will not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload sample training data to S3:\n",
    "\n",
    "### Note:\n",
    "\n",
    "You'll need your data in a .csv or .jsonl file that contains prompt-completion pairs as your examples.\n",
    "\n",
    "\n",
    "### Example:\n",
    "\n",
    "JSONL:  `{\"prompt\": \"This is the first prompt\", \"completion\": \"This is the first completion\"}`\n",
    "\n",
    "CSV:  `\"This is the first prompt\" , \"This is the first completion\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sage.Session()\n",
    "train_dataset = S3Uploader.upload(\"../examples/sample_sst5_finetuning_data.jsonl\", s3_data_dir, sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Repeat for the same for the evaluation dataset if you have one. If absent, we will auto-split the training dataset into training and evaluation datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Finetune model on uploaded data\n",
    "\n",
    "Specify a directory on S3 where finetuned models should be stored. Make sure you do not reuse the same directory across multiple runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT re-use the same s3 directory for multiple finetunes\n",
    "s3_models_dir = \"s3://...\"  # Do not add a trailing slash otherwise it will not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Cohere client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = Client(region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Define hyperparameters\n",
    "\n",
    "- train_epochs: This is the maximum number of training epochs to run for. Defaults to **1**.\n",
    "- strategy: Use either **tfew** or **vanilla**. Defaults to **tfew**, a parameter efficient finetuning approach. **vanilla** implies the weight updates will be applied to last half of the layers for Command-light and last quarter of the layers for Command.\n",
    "- learning_rate: The initial learning rate to be used during training. Default differs based on ARN and strategy and is listed below.\n",
    "- train_batch_size: The batch size used during training. Defaults to **16** for Command and **8** for Command-light.\n",
    "- early_stopping_patience: Stop training if the loss metric does not improve beyond 'early_stopping_threshold' for this many times of evaluation. Defaults to **6.**\n",
    "- early_stopping_threshold: How much the loss must improve to prevent early stopping. Defaults to **0.01**.\n",
    "\n",
    "\n",
    "| Model | Strategy | Learning Rate |\n",
    "| --- | --- | --- |\n",
    "| Command-light | vanilla | 6E-07 |\n",
    "| Command-light | tfew | 0.01 |\n",
    "| Command | vanilla | 1E-05 |\n",
    "| Command | tfew | 0.01 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to pass hyperparameters to the fine-tuning job\n",
    "train_parameters = {\n",
    "    \"train_epochs\": 1,\n",
    "    \"strategy\": \"tfew\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"train_batch_size\": 16,\n",
    "    \"early_stopping_patience\": 5,\n",
    "    \"early_stopping_threshold\": 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create fine-tuning jobs for the uploaded datasets. Add a field for `eval_data` if you have pre-split your dataset and uploaded both training and evaluation datasets to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take approximately 30 minutes with the example dataset\n",
    "co.create_finetune(arn=arn,\n",
    "    name=\"test-custom-finetune\",\n",
    "    train_data=train_dataset,\n",
    "    s3_models_dir=s3_models_dir,\n",
    "    instance_type=\"ml.p4de.24xlarge\",\n",
    "    training_parameters=train_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create an endpoint for inference with the custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Create an endpoint\n",
    "\n",
    "The Cohere SageMaker SDK provides a built-in method for creating an endpoint for inference. This will automatically deploy the model you finetuned earlier.\n",
    "\n",
    "> **Note**: This is equivalent to creating and deploying a `ModelPackage` in SageMaker's SDK.\n",
    "\n",
    "You can serve multiple t-few finetunes if you store all of them in the same S3 directory and pass the directory as `s3_model_dir`. Please note that you should use dedicated directories for vanilla finetunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co.create_endpoint(arn=arn,\n",
    "    endpoint_name=\"command-light-finetune-test\",\n",
    "    s3_models_dir=s3_models_dir,\n",
    "    recreate=True,\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    ")\n",
    "\n",
    "# # If the endpoint is already created, you just need to connect to it\n",
    "# co.connect_to_endpoint(endpoint_name=\"cohere-classify\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Perform real-time inference\n",
    "\n",
    "Now, you can access all models deployed on the endpoint for inference:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When serving t-few finetunes, you must additionally pass the `name` in the `model` parameter as specified during the creation of the finetune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Classify the following text as either very negative, negative, neutral, positive or very positive: mr. deeds is , as comedy goes , very silly -- and in the best way.\"\n",
    "\n",
    "# vanilla\n",
    "# result = co.generate(prompt=prompt, max_tokens=50)\n",
    "\n",
    "# tfew\n",
    "result = co.generate(model=\"test-custom-finetune\", prompt=prompt, max_tokens=50)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the endpoint\n",
    "\n",
    "After you've successfully performed inference, you can delete the deployed endpoint to avoid being charged continuously. This can also be done via the Cohere SageMaker SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co.delete_endpoint()\n",
    "co.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Unsubscribe to the listing (optional)\n",
    "\n",
    "If you would like to unsubscribe to the model package, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b57b3736fb00bc0deb03789040183ddbda4c9eb8e8f6bef7ea4333bc64826af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
